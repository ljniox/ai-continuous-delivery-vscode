#!/usr/bin/env python3
"""
Script DoD Gate - Valide les critÃ¨res de Definition of Done
et dÃ©cide si le sprint peut Ãªtre mergÃ©
"""

import os
import json
from pathlib import Path
from supabase import create_client, Client

def evaluate_dod(summary, dod_criteria):
    """
    Ã‰value si les critÃ¨res DoD sont respectÃ©s
    """
    results = {
        'passed': True,
        'details': [],
        'score': 0,
        'total_criteria': 0
    }
    
    # Coverage minimum
    if 'coverage_min' in dod_criteria:
        coverage_min = dod_criteria['coverage_min']
        actual_coverage = summary.get('coverage', 0)
        
        if actual_coverage >= coverage_min:
            results['details'].append(f"âœ… Coverage: {actual_coverage:.1%} >= {coverage_min:.1%}")
            results['score'] += 1
        else:
            results['details'].append(f"âŒ Coverage: {actual_coverage:.1%} < {coverage_min:.1%}")
            results['passed'] = False
        results['total_criteria'] += 1
    
    # Tests E2E passent
    if dod_criteria.get('e2e_pass', False):
        e2e_pass = summary.get('e2e_pass', False)
        if e2e_pass:
            results['details'].append("âœ… Tests E2E: PassÃ©s")
            results['score'] += 1
        else:
            results['details'].append("âŒ Tests E2E: Ã‰checs")
            results['passed'] = False
        results['total_criteria'] += 1
    
    # Tests unitaires passent
    unit_pass = summary.get('unit_pass', False)
    if unit_pass:
        results['details'].append("âœ… Tests unitaires: PassÃ©s")
        results['score'] += 1
    else:
        results['details'].append("âŒ Tests unitaires: Ã‰checs")
        results['passed'] = False
    results['total_criteria'] += 1
    
    # Score Lighthouse minimum
    if 'lighthouse_min' in dod_criteria:
        lighthouse_min = dod_criteria['lighthouse_min']
        actual_lighthouse = summary.get('lighthouse', 0)
        
        if actual_lighthouse >= lighthouse_min:
            results['details'].append(f"âœ… Lighthouse: {actual_lighthouse} >= {lighthouse_min}")
            results['score'] += 1
        else:
            results['details'].append(f"âŒ Lighthouse: {actual_lighthouse} < {lighthouse_min}")
            results['passed'] = False
        results['total_criteria'] += 1
    
    return results

def main():
    # Variables d'environnement
    supabase_url = os.getenv('SUPABASE_URL')
    supabase_key = os.getenv('SUPABASE_SERVICE_KEY')
    run_id = os.getenv('RUN_ID')
    
    if not supabase_url or not supabase_key or not run_id:
        print("âŒ Variables SUPABASE_URL, SUPABASE_SERVICE_KEY ou RUN_ID manquantes")
        exit(1)
    
    # Connexion Supabase
    supabase: Client = create_client(supabase_url, supabase_key)
    
    # Charger le rÃ©sumÃ© des tests
    summary_file = Path('artifacts/summary.json')
    if not summary_file.exists():
        print("âŒ Fichier artifacts/summary.json non trouvÃ©")
        exit(1)
    
    with open(summary_file, 'r') as f:
        summary = json.load(f)
    
    print(f"ğŸ” Ã‰valuation DoD pour le run {run_id}")
    
    try:
        # RÃ©cupÃ©rer les informations du run et du sprint
        run_data = supabase.table('runs').select('*, sprints(*)').eq('id', run_id).single().execute()
        
        if not run_data.data:
            print("âŒ Run non trouvÃ©")
            exit(1)
        
        run = run_data.data
        sprint = run['sprints']
        dod_criteria = sprint['dod_json']
        
        print(f"ğŸ“‹ Sprint: {sprint['label']}")
        print(f"ğŸ“Š CritÃ¨res DoD: {json.dumps(dod_criteria, indent=2)}")
        
        # Ã‰valuer les critÃ¨res
        evaluation = evaluate_dod(summary, dod_criteria)
        
        print(f"\nğŸ¯ RÃ©sultat DoD: {evaluation['score']}/{evaluation['total_criteria']}")
        for detail in evaluation['details']:
            print(f"   {detail}")
        
        # Mettre Ã  jour le rÃ©sumÃ©
        summary['dod_evaluation'] = evaluation
        summary['result'] = 'PASSED' if evaluation['passed'] else 'FAILED'
        
        # Sauvegarder le rÃ©sumÃ© mis Ã  jour
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Mettre Ã  jour le run dans la base
        run_update = {
            'finished_at': 'now()',
            'result': summary['result'],
            'summary_json': summary
        }
        
        supabase.table('runs').update(run_update).eq('id', run_id).execute()
        
        # Mettre Ã  jour le statut du sprint
        sprint_state = 'DONE' if evaluation['passed'] else 'FAILED'
        supabase.table('sprints').update({'state': sprint_state}).eq('id', sprint['id']).execute()
        
        # Enregistrer un Ã©vÃ©nement de statut final
        final_phase = 'TESTS_PASSED' if evaluation['passed'] else 'TESTS_FAILED'
        supabase.table('status_events').insert({
            'run_id': run_id,
            'phase': final_phase,
            'message': f"DoD {summary['result']}: {evaluation['score']}/{evaluation['total_criteria']} critÃ¨res"
        }).execute()
        
        if evaluation['passed']:
            print("\nâœ… DoD PASSÃ‰ - Sprint prÃªt pour merge")
            exit(0)
        else:
            print("\nâŒ DoD Ã‰CHOUÃ‰ - Corrections nÃ©cessaires")
            exit(1)
            
    except Exception as e:
        print(f"âŒ Erreur lors de l'Ã©valuation DoD: {e}")
        
        # Marquer comme Ã©chec en cas d'erreur
        summary['result'] = 'FAILED'
        summary['error'] = str(e)
        
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        supabase.table('runs').update({
            'finished_at': 'now()',
            'result': 'FAILED',
            'summary_json': summary
        }).eq('id', run_id).execute()
        
        exit(1)

if __name__ == '__main__':
    main()